{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ybercovich/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import scipy.io\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from six.moves import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def download(download_link, file_name, expected_bytes):\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"pre-trained model ready\")\n",
    "        return\n",
    "    print(\"Downloading the pre-trained model. This might take a while ...\")\n",
    "    file_name, _ = urllib.request.urlretrieve(download_link, file_name)\n",
    "    file_stat = os.stat(file_name)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded  pre-trained model', file_name)\n",
    "    else:\n",
    "        raise Exception('File ' + file_name +\n",
    "                        ' might be corrupted. You should try downloading it with a browser.')\n",
    "        \n",
    "\n",
    "def get_resized_image(img_path, height, width, save=True):\n",
    "    image = Image.open(img_path)\n",
    "    # it's because PIL is column major so you have to change place of width and height\n",
    "    # this is stupid, i know\n",
    "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
    "    if save:\n",
    "        image_dirs = img_path.split('/')\n",
    "        image_dirs[-1] = 'resized_' + image_dirs[-1]\n",
    "        out_path = '/'.join(image_dirs)\n",
    "        if not os.path.exists(out_path):\n",
    "            image.save(out_path)\n",
    "    image = np.asarray(image, np.float32)\n",
    "    return np.expand_dims(image, 0)\n",
    "\n",
    "def generate_noise_image(content_image, height, width, noise_ratio=0.6):\n",
    "    noise_image = np.random.uniform(-20, 20,\n",
    "                                    (1, height, width, 3)).astype(np.float32)\n",
    "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "def save_image(path, image):\n",
    "    # Output should add back the mean pixels we subtracted at the beginning\n",
    "    image = image[0] # the image\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD ALEX NET MODEL\n",
    "def load_image(filename, max_size=None):\n",
    "    image = Image.open(filename)\n",
    "\n",
    "    if max_size is not None:\n",
    "        # Calculate the appropriate rescale-factor for\n",
    "        # ensuring a max height and width, while keeping\n",
    "        # the proportion between them.\n",
    "        factor = max_size / np.max(image.size)\n",
    "\n",
    "        # Scale the image's height and width.\n",
    "        size = np.array(image.size) * factor\n",
    "\n",
    "        # The size is now floating-point because it was scaled.\n",
    "        # But PIL requires the size to be integers.\n",
    "        size = size.astype(int)\n",
    "\n",
    "        # Resize the image.\n",
    "        image = image.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Convert to numpy floating-point array.\n",
    "    return np.float32(image)\n",
    "\n",
    "\n",
    "def _weights(trained_layers, layer_idx, expected_layer_name):\n",
    "    \"\"\" Return the weights and biases already trained by the net \"\"\"\n",
    "    W = trained_layers[0][layer_idx][0][0][2][0][0]\n",
    "    b = trained_layers[0][layer_idx][0][0][2][0][1]\n",
    "    layer_name = trained_layers[0][layer_idx][0][0][0][0]\n",
    "\n",
    "    assert layer_name == expected_layer_name\n",
    "\n",
    "    # if (layer_idx == 0): put this in comment to fix for laoding alex net\n",
    "    b = b.reshape(b.size)\n",
    "\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def _conv2d_relu_2(trained_layers, prev_layer, layer_idx, layer_name, s_h, s_w, group, padding='SAME'):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/ethereon/caffe-tensorflow\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w, b = _weights(trained_layers, layer_idx, layer_name)\n",
    "        w = tf.constant(w, name='weights')\n",
    "        b = tf.constant(b, name='bias')\n",
    "\n",
    "        convolve = lambda inputs, weigths: tf.nn.conv2d(inputs, weigths, [1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "        if group == 1:\n",
    "            conv2d = convolve(prev_layer, w)\n",
    "        else:\n",
    "            # Split input and weights and conv2d them separately\n",
    "            input_groups = tf.split(prev_layer, group, 3)\n",
    "            kernel_groups = tf.split(w, group, 3)\n",
    "            output_groups = [tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "                             for i, k in zip(input_groups, kernel_groups)]\n",
    "            conv2d = tf.concat(output_groups, 3)\n",
    "\n",
    "        conv_lin = tf.reshape(tf.nn.bias_add(conv2d, b), [-1] + conv2d.get_shape().as_list()[1:], name='lin')\n",
    "        conv_relu = tf.nn.relu(conv_lin, name='relu')\n",
    "\n",
    "        return conv_relu\n",
    "\n",
    "\n",
    "def _maxpool(prev_layer, layer_name):\n",
    "    return tf.nn.max_pool(prev_layer, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name=layer_name)\n",
    "\n",
    "\n",
    "def _avgpool(prev_layer, layer_name):\n",
    "    return tf.nn.avg_pool(prev_layer, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name=layer_name)\n",
    "\n",
    "\n",
    "def _lrn(prev_layer, layer_name):\n",
    "    return tf.nn.local_response_normalization(prev_layer, depth_radius=2, alpha=2e-05, beta=0.75, bias=1.0,\n",
    "                                              name=layer_name)\n",
    "\n",
    "\n",
    "def load_net_from_file(path, input_image):\n",
    "    \"\"\" Load net into a TensorFlow model. Use a dictionary to hold the model instead of using a Python class \"\"\"\n",
    "    net = scipy.io.loadmat(path)\n",
    "    trained_layers = net['layers']\n",
    "\n",
    "    graph = {}\n",
    "    graph['conv1'] = _conv2d_relu_2(trained_layers, input_image, 0, 'conv1', s_h=4, s_w=4, group=1)\n",
    "    graph['norm1'] = _lrn(graph['conv1'], 'norm1')\n",
    "    graph['pool1'] = _avgpool(graph['norm1'], 'pool1')\n",
    "\n",
    "    graph['conv2'] = _conv2d_relu_2(trained_layers, graph['pool1'], 4, 'conv2', s_h=1, s_w=1, group=2)\n",
    "    graph['norm2'] = _lrn(graph['conv2'], 'norm2')\n",
    "    graph['pool2'] = _avgpool(graph['norm2'], 'pool2')\n",
    "\n",
    "    graph['conv3'] = _conv2d_relu_2(trained_layers, graph['pool2'], 8, 'conv3', s_h=1, s_w=1, group=1)\n",
    "    graph['conv4'] = _conv2d_relu_2(trained_layers, graph['conv3'], 10, 'conv4', s_h=1, s_w=1, group=2)\n",
    "    graph['conv5'] = _conv2d_relu_2(trained_layers, graph['conv4'], 12, 'conv5', s_h=1, s_w=1, group=2)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "# THIS IS THE END OF ALEXNET STYLE TRANSFER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" An implementation of the paper \"A Neural Algorithm of Artistic Style\"\n",
    "by Gatys et al. in TensorFlow.\n",
    "\n",
    "Author: Chip Huyen (huyenn@stanford.edu)\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "For more details, please read the assignment handout:\n",
    "http://web.stanford.edu/class/cs20si/assignments/a2.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# new alex net\n",
    "DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-caffe-alex.mat'\n",
    "MODEL = 'imagenet-caffe-alex.mat'\n",
    "EXPECTED_BYTES = 228041398\n",
    "\n",
    "STYLE = 'vanGogh'\n",
    "CONTENT = 'maayan'\n",
    "STYLE_IMAGE = '/Users/ybercovich/Documents/mta_tensorflow/assignment2/images/' + STYLE + '.jpg'\n",
    "CONTENT_IMAGE = '/Users/ybercovich/Documents/mta_tensorflow/assignment2/images/' + CONTENT + '.jpg'\n",
    "\n",
    "#alexnet is trained for 227 * 227 sized images\n",
    "IMAGE_HEIGHT = 500 #227\n",
    "IMAGE_WIDTH = 500 #227\n",
    "\n",
    "CONTENT_WEIGHT = 0.50 #0.01\n",
    "STYLE_WEIGHT = 1000 #1\n",
    "\n",
    "NOISE_RATIO = 0.6  # percentage of weight of the noise for intermixing with the content image\n",
    "\n",
    "\n",
    "# Layers used for style features. You can change this.\n",
    "#STYLE_LAYERS = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5']\n",
    "STYLE_LAYERS = ['conv1', 'conv3', 'conv4', 'conv5']\n",
    "W = [0.5, 1.5, 3.0, 4.0]  # give more weights to deeper layers.\n",
    "\n",
    "# Layer used for content features. You can change this.\n",
    "CONTENT_LAYER = 'conv2'\n",
    "\n",
    "ITERS = 600\n",
    "LR = 2.0\n",
    "\n",
    "MEAN_PIXELS = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n",
    "\"\"\" MEAN_PIXELS is defined according to description on their github:\n",
    "https://gist.github.com/ksimonyan/211839e770f7b538e2d8\n",
    "'In the paper, the model is denoted as the configuration D trained with scale jittering. \n",
    "The input images should be zero-centered by mean pixel (rather than mean image) subtraction. \n",
    "Namely, the following BGR values should be subtracted: [103.939, 116.779, 123.68].'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# VGG-19 parameters file\n",
    "#VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "#VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "#EXPECTED_BYTES = 534904783\n",
    "\n",
    "\n",
    "def _create_content_loss(p, f):\n",
    "    \"\"\" Calculate the loss between the feature representation of the\n",
    "    content image and the generated image.\n",
    "\n",
    "    Inputs:\n",
    "        p, f are just P, F in the paper\n",
    "        (read the assignment handout if you're confused)\n",
    "        Note: we won't use the coefficient 0.5 as defined in the paper\n",
    "        but the coefficient as defined in the assignment handout.\n",
    "    Output:\n",
    "        the content loss\n",
    "\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum((f - p) ** 2) / (4.0 * p.size)\n",
    "\n",
    "\n",
    "def _gram_matrix(F, N, M):\n",
    "    \"\"\" Create and return the gram matrix for tensor F\n",
    "        Hint: you'll first have to reshape F\n",
    "    \"\"\"\n",
    "    F = tf.reshape(F, (M, N))\n",
    "    return tf.matmul(tf.transpose(F), F)\n",
    "\n",
    "\n",
    "def _single_style_loss(a, g):\n",
    "    \"\"\" Calculate the style loss at a certain layer\n",
    "    Inputs:\n",
    "        a is the feature representation of the real image\n",
    "        g is the feature representation of the generated image\n",
    "    Output:\n",
    "        the style loss at a certain layer (which is E_l in the paper)\n",
    "\n",
    "    Hint: 1. you'll have to use the function _gram_matrix()\n",
    "        2. we'll use the same coefficient for style loss as in the paper\n",
    "        3. a and g are feature representation, not gram matrices\n",
    "    \"\"\"\n",
    "    N = a.shape[3]  # number of filters\n",
    "    M = a.shape[1] * a.shape[2]  # height times width of the feature map\n",
    "    A = _gram_matrix(a, N, M)\n",
    "    G = _gram_matrix(g, N, M)\n",
    "    return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))\n",
    "\n",
    "\n",
    "def _create_style_loss(A, model):\n",
    "    \"\"\" Return the total style loss\n",
    "    \"\"\"\n",
    "    n_layers = len(STYLE_LAYERS)\n",
    "    E = [_single_style_loss(A[i], model[STYLE_LAYERS[i]]) for i in range(n_layers)]\n",
    "\n",
    "    return sum([W[i] * E[i] for i in range(n_layers)])\n",
    "\n",
    "\n",
    "def _create_losses(model, input_image, content_image, style_image):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(input_image.assign(content_image))  # assign content image to the input variable\n",
    "            p = sess.run(model[CONTENT_LAYER])\n",
    "        content_loss = _create_content_loss(p, model[CONTENT_LAYER])\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(input_image.assign(style_image))\n",
    "            A = sess.run([model[layer_name] for layer_name in STYLE_LAYERS])\n",
    "        style_loss = _create_style_loss(A, model)\n",
    "\n",
    "\n",
    "        total_loss = CONTENT_WEIGHT * content_loss + STYLE_WEIGHT * style_loss\n",
    "\n",
    "\n",
    "    return content_loss, style_loss, total_loss\n",
    "\n",
    "\n",
    "def _create_summary(model):\n",
    "    \"\"\" Create summary ops necessary\n",
    "        Hint: don't forget to merge them\n",
    "    \"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('content loss', model['content_loss'])\n",
    "        tf.summary.scalar('style loss', model['style_loss'])\n",
    "        tf.summary.scalar('total loss', model['total_loss'])\n",
    "        tf.summary.histogram('histogram content loss', model['content_loss'])\n",
    "        tf.summary.histogram('histogram style loss', model['style_loss'])\n",
    "        tf.summary.histogram('histogram total loss', model['total_loss'])\n",
    "        return tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, generated_image, initial_image):\n",
    "    \"\"\" Train your model.\n",
    "    Don't forget to create folders for checkpoints and outputs.\n",
    "    \"\"\"\n",
    "    skip_step = 1\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "\n",
    "        sess.run(generated_image.assign(initial_image))\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        initial_step = model['global_step'].eval()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for index in range(initial_step, ITERS):\n",
    "            if index >= 5 and index < 20:\n",
    "                skip_step = 10\n",
    "            elif index >= 20:\n",
    "                skip_step = 20\n",
    "\n",
    "            sess.run(model['optimizer'])\n",
    "            if (index + 1) % skip_step == 0:\n",
    "\n",
    "                gen_image, total_loss, summary = sess.run([generated_image, model['total_loss'],\n",
    "                                                           model['summary_op']])\n",
    "\n",
    "                gen_image = gen_image + MEAN_PIXELS\n",
    "                writer.add_summary(summary, global_step=index)\n",
    "                print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "                print('   Loss: {:5.1f}'.format(total_loss))\n",
    "                print('   Time: {}'.format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "\n",
    "                filename = 'outputs/%d.png' % (index)\n",
    "\n",
    "                save_image(filename, gen_image)\n",
    "\n",
    "                if (index + 1) % 20 == 0:\n",
    "                    saver.save(sess, 'checkpoints/style_transfer', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    with tf.variable_scope('input') as scope:\n",
    "        # use variable instead of placeholder because we're training the intial image to make it\n",
    "        # look like both the content image and the style image\n",
    "        input_image = tf.Variable(np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3]), dtype=tf.float32)\n",
    "\n",
    "    download(DOWNLOAD_LINK, MODEL, EXPECTED_BYTES)\n",
    "\n",
    "    make_dir('checkpoints')\n",
    "    make_dir('outputs')\n",
    "\n",
    "    model = load_net_from_file(MODEL, input_image)\n",
    "    model['global_step'] = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    content_image = get_resized_image(CONTENT_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    content_image = content_image - MEAN_PIXELS\n",
    "\n",
    "    style_image = get_resized_image(STYLE_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    style_image = style_image - MEAN_PIXELS\n",
    "\n",
    "    model['content_loss'], model['style_loss'], model['total_loss'] = _create_losses(model,\n",
    "                                                                                     input_image, content_image,\n",
    "                                                                                     style_image)\n",
    "\n",
    "    model['optimizer'] = tf.train.AdamOptimizer(LR).minimize(model['total_loss'],\n",
    "                                                             global_step=model['global_step'])\n",
    "    model['summary_op'] = _create_summary(model)\n",
    "\n",
    "    initial_image = generate_noise_image(content_image, IMAGE_HEIGHT, IMAGE_WIDTH, NOISE_RATIO)\n",
    "    train(model, input_image, initial_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained model ready\n",
      "INFO:tensorflow:Summary name content loss is illegal; using content_loss instead.\n",
      "INFO:tensorflow:Summary name style loss is illegal; using style_loss instead.\n",
      "INFO:tensorflow:Summary name total loss is illegal; using total_loss instead.\n",
      "INFO:tensorflow:Summary name histogram content loss is illegal; using histogram_content_loss instead.\n",
      "INFO:tensorflow:Summary name histogram style loss is illegal; using histogram_style_loss instead.\n",
      "INFO:tensorflow:Summary name histogram total loss is illegal; using histogram_total_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/style_transfer-299\n",
      "Step 320\n",
      "   Sum: 109148014.9\n",
      "   Loss: 28253.2\n",
      "   Time: 8.613300323486328\n",
      "Step 340\n",
      "   Sum: 109184894.3\n",
      "   Loss: 25463.3\n",
      "   Time: 8.366456031799316\n",
      "Step 360\n",
      "   Sum: 109219141.8\n",
      "   Loss: 23065.3\n",
      "   Time: 9.030625104904175\n",
      "Step 380\n",
      "   Sum: 109251296.4\n",
      "   Loss: 20978.7\n",
      "   Time: 8.804209232330322\n",
      "Step 400\n",
      "   Sum: 109281694.7\n",
      "   Loss: 19148.9\n",
      "   Time: 9.847063064575195\n",
      "Step 420\n",
      "   Sum: 109310685.5\n",
      "   Loss: 17538.0\n",
      "   Time: 9.75127124786377\n",
      "Step 440\n",
      "   Sum: 109338414.1\n",
      "   Loss: 16109.3\n",
      "   Time: 9.18818712234497\n",
      "Step 460\n",
      "   Sum: 109364959.3\n",
      "   Loss: 14835.3\n",
      "   Time: 10.217228174209595\n",
      "Step 480\n",
      "   Sum: 109390407.5\n",
      "   Loss: 13696.5\n",
      "   Time: 9.939017057418823\n",
      "Step 500\n",
      "   Sum: 109414856.5\n",
      "   Loss: 12674.0\n",
      "   Time: 9.649703025817871\n",
      "Step 520\n",
      "   Sum: 109438427.8\n",
      "   Loss: 11753.7\n",
      "   Time: 9.091811180114746\n",
      "Step 540\n",
      "   Sum: 109461174.4\n",
      "   Loss: 10923.1\n",
      "   Time: 10.173380136489868\n",
      "Step 560\n",
      "   Sum: 109483013.1\n",
      "   Loss: 10171.2\n",
      "   Time: 9.465757369995117\n",
      "Step 580\n",
      "   Sum: 109504056.6\n",
      "   Loss: 9489.9\n",
      "   Time: 9.495497941970825\n",
      "Step 600\n",
      "   Sum: 109524414.4\n",
      "   Loss: 8872.1\n",
      "   Time: 9.103777170181274\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
